{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning with Python Ch8: Nietzsche\n",
    "#####################\n",
    "#  text generation  #\n",
    "#####################\n",
    "# doawload and parse original text file to lowercase\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "\n",
    "path = kr.utils.get_file('nietzsche.txt', origin='http://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path, encoding='utf8').read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sequences: 200278\n",
      "# of unique characters: 57\n"
     ]
    }
   ],
   "source": [
    "# vectorize sequences using one-hot encoding\n",
    "maxlen = 60 # # of characters to extract per sequence\n",
    "step = 3    # sample a new sequence every 3 characters\n",
    "\n",
    "sentences = [] # hold extracted sequence\n",
    "nextchars = [] # hold follow-up characters (or targets)\n",
    "for i in range(0, len(text)-maxlen, step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    nextchars.append(text[i+maxlen])\n",
    "print('# of sequences:', len(sentences))\n",
    "\n",
    "# list of unique characters in corpus\n",
    "chars        = sorted(list(set(text)))\n",
    "# dictionary maps each unique character to its index in list chars\n",
    "char_indices = dict((char, chars.index(char)) for char in chars) \n",
    "print('# of unique characters:', len(chars))\n",
    "\n",
    "# one-hot encoding\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]]  = 1\n",
    "    y[i, char_indices[nextchars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               95232     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 57)                7353      \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build network with LSTM to predict next character\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = optimizers.RMSprop(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 191s 954us/step - loss: 1.9785\n",
      "--- generating with seed: \" lived; so that at\n",
      "first the old motives of vehement passion  \" \n",
      " \n",
      "------ temperature: 0.2\n",
      "lived; so that at\n",
      "first the old motives of vehement passion of the strued and the strength in the strenged of the suppicion of a so the signing the strength, an \n",
      " \n",
      "------ temperature: 0.5\n",
      "lived; so that at\n",
      "first the old motives of vehement passion that it we soul and strect of comple and of the person the self-distring mas to methonghous to so a  \n",
      " \n",
      "------ temperature: 1.0\n",
      "lived; so that at\n",
      "first the old motives of vehement passion howed the hows see bath delicide? serive and a aysord find, conraraling, the empe ten ervatayed crad \n",
      " \n",
      "------ temperature: 1.2\n",
      "lived; so that at\n",
      "first the old motives of vehement passion the mo sumpiritabul\n",
      "cimine,\n",
      "churotion\n",
      "charding, they refulapheroded develictas. far orrdinncogon rel \n",
      "epoch # 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 188s 940us/step - loss: 1.6276\n",
      "--- generating with seed: \"  the approval of all the positivists of\n",
      "france and germany ( \" \n",
      " \n",
      "------ temperature: 0.2\n",
      " the approval of all the positivists of\n",
      "france and germany (and the souls the souls to the sought of the stronger of the souls the stronger of the moral of the  \n",
      " \n",
      "------ temperature: 0.5\n",
      " the approval of all the positivists of\n",
      "france and germany (the work of the possions of the away the partian of the work of the work of the one to power the bri \n",
      " \n",
      "------ temperature: 1.0\n",
      " the approval of all the positivists of\n",
      "france and germany (indared moraling\n",
      "neverty\n",
      "8ause-age of withther not overspecised where is the which on they the worth \n",
      " \n",
      "------ temperature: 1.2\n",
      " the approval of all the positivists of\n",
      "france and germany (soffary with\n",
      "by can interpoant with\n",
      "unwagal,\" of\n",
      "wititulops--in greatfrougg and,ge very all\n",
      "perhapat \n"
     ]
    }
   ],
   "source": [
    "# train network and generate text from it\n",
    "# fn to sample the next character given the model's prediction\n",
    "# it reweights the original probability distribution the model outputs\n",
    "# and draws a character index from the newly-weighted distribution\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds     = np.asarray(preds).astype('float64')\n",
    "    preds     = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds     = exp_preds / np.sum(exp_preds)\n",
    "    probs     = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probs)\n",
    "\n",
    "# train network and generate text using it\n",
    "import random, sys\n",
    "\n",
    "# only train the model for 2 epochs to save time\n",
    "for epoch in range(1, 3): \n",
    "    print('epoch #', epoch)\n",
    "    # fit model for one iteration of the data\n",
    "    model.fit(x, y, batch_size=128, epochs=1) \n",
    "    \n",
    "    # randomly select a text seed\n",
    "    start_index = random.randint(0, len(text)-maxlen-1)\n",
    "    seed_text   = text[start_index : start_index+maxlen]\n",
    "    print('--- generating with seed: \" '+ seed_text + ' \" ')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(' ')\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(seed_text)\n",
    "        \n",
    "        # generate 100 characters starting from the seed text\n",
    "        generated_text = seed_text\n",
    "        for i in range(100):\n",
    "            # one-hot encodes the character generated so far\n",
    "            sampled = np.zeros([1, maxlen, len(chars)])\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1\n",
    "            \n",
    "            # sample the next character\n",
    "            preds      = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char  = chars[next_index]\n",
    "            \n",
    "            generated_text += next_char\n",
    "            generated_text  = generated_text[1:]\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "        print(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras-gpu]",
   "language": "python",
   "name": "conda-env-keras-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
